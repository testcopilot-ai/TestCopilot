# Replication package for paper "Scenario-Driven Test Case Generation withAutonomous Agents"

![Architecture](TestCopilot.png)
# TestCopilot
Scenario-enriched LLM-based framework for automatic test case generation, bug detection, and code evaluation.

Scenario-Driven Test Case Generation with Autonomous Agents

# ğŸš€ Overview

TestCopilot is a multi-agent framework designed to automate software test case generation using Large Language Models (LLMs). It integrates scenario-enriched prompting, bug detection, and coverage analysis to produce high-quality and maintainable test suites. The system is benchmarked on HumanEval and MBPP, demonstrating state-of-the-art performance across correctness, coverage, and maintainability metrics.


# ğŸ“Š  Performance Highlights

```plaintext
TestCopilot significantly outperforms existing models:
Metric	TestCopilot
TCE (HumanEval)	99.3%
DDP (HumanEval)	99.7%
Function Coverage 89.5%
Bugs Detected (HumanEval) 179
Maintainability Index 81.31%
False Alarms 0
```
ğŸ”‘ Key Features
# âœ… Scenario-Enriched Prompting

Integrates functional requirements and user stories to guide LLMs in generating purpose-driven test cases.
ğŸ§  Multi-Agent Evaluation

Includes separate agents for generation, bug detection, test refinement, and repair validation.
ğŸ“ˆ Deep Coverage Analysis

Calculates statement, branch, path, and integration coverage along with maintainability scores.
ğŸ”„ Test Repair Feedback Loop

Fixes incomplete or incorrect test cases using iterative improvement agents.
```plaintext
TestCopilot/
â”‚
â”œâ”€â”€ ğŸ“‚ dataset/ # HumanEval / MBPP benchmark datasets
â”‚ â”œâ”€â”€ HumanEval_Scenario_testcases.xlsx
â”‚ â”œâ”€â”€ MBPP_Scenario_testcases.xlsx
â”‚
â”œâ”€â”€ ğŸ“‚ LLM-Based Evaluation/ # Multi-agent evaluation & robustness
â”‚ â”œâ”€â”€ compute_repair_vs_discard.py # Repair vs discard-fail comparison
â”‚ â”œâ”€â”€ compute_temp_token.py # Temp/token variation analysis
â”‚ â”œâ”€â”€ mainchatgpt.py # Evaluation with ChatGPT
â”‚ â”œâ”€â”€ maindeepseek.py # Evaluation with DeepSeek
â”‚ â”œâ”€â”€ reasoningandnonreasoning.py # Reasoning vs non-reasoning analysis
â”‚ â”œâ”€â”€ semantic_fidelity.py # Semantic fidelity evaluation
â”‚ â”œâ”€â”€ stats_robustness.py # Statistical robustness analysis
â”‚
â”œâ”€â”€ ğŸ“‚ baseline/ # Baseline evaluations & metrics
â”‚ â”œâ”€â”€ main.py
â”‚ â”œâ”€â”€ mainaibugy.py
â”‚ â”œâ”€â”€ mainbugsapproach.py
â”‚ â”œâ”€â”€ maincompute_pyuguinmetrics.py
â”‚ â”œâ”€â”€ maincoveragezero.py
â”‚ â”œâ”€â”€ mainmaintainabilty.py
â”‚ â”œâ”€â”€ mainpyuguin.py
â”‚ â”œâ”€â”€ mainpyuguin_mutation.py
â”‚ â”œâ”€â”€ mainstatandfunccov.py
â”‚
â”œâ”€â”€ ğŸ“‚ scenariogenerated/ # Scenario generation pipeline
â”‚ â”œâ”€â”€ main.py
â”‚
â”œâ”€â”€ .env # API keys (OpenAI, DeepSeek)
â”œâ”€â”€ requirements.txt # Dependencies
â”œâ”€â”€ README.md # Documentation
|
```

# ğŸ“Œ Requirements
ğŸ–¥ï¸ System

    Python 3.9+

    Internet access for LLM API calls

Install them with:

pip install -r requirements.txt

## Running TestCopilot

To run TestCopilot on your dataset, use the following command:

```
Step#1
python scenariogenerated/main.py \
  --input dataset/HumanEval_Scenario_testcases.xlsx \
  --output outputs/scenarios

Step#2
python "LLM-Based Evaluation/mainchatgpt.py" \
  --input outputs/scenarios \
  --output outputs/evaluated_tests

```

