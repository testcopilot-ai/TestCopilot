# Replication package for paper "Scenario-Driven Test Case Generation withAutonomous Agents"

![Architecture](TestCopilot.png)
# TestCopilot
Scenario-enriched LLM-based framework for automatic test case generation, bug detection, and code evaluation.

Scenario-Driven Test Case Generation with Autonomous Agents

# ğŸš€ Overview

TestCopilot is a multi-agent framework designed to automate software test case generation using Large Language Models (LLMs). It integrates scenario-enriched prompting, bug detection, and coverage analysis to produce high-quality and maintainable test suites. The system is benchmarked on HumanEval and MBPP, demonstrating state-of-the-art performance across correctness, coverage, and maintainability metrics.


# ğŸ“Š  Performance Highlights

```plaintext
TestCopilot significantly outperforms existing models:
Metric	TestCopilot
TCE (HumanEval)	99.3%
DDP (HumanEval)	 99.7%
Function Coverage	89.5%
Bugs Detected (HumanEval)	179
False Alarms	0
```
ğŸ”‘ Key Features
# âœ… Scenario-Enriched Prompting

Integrates functional requirements and user stories to guide LLMs in generating purpose-driven test cases.
ğŸ§  Multi-Agent Evaluation

Includes separate agents for generation, bug detection, test refinement, and repair validation.
ğŸ“ˆ Deep Coverage Analysis

Calculates statement, branch, path, and integration coverage along with maintainability scores.
ğŸ”„ Test Repair Feedback Loop

Fixes incomplete or incorrect test cases using iterative improvement agents.
```plaintext
ğŸ“¦ TestCopilot/
â”‚
â”œâ”€â”€ ğŸ“‚ dataset/                  # HumanEval / MBPP test scenario datasets
â”‚   â”œâ”€â”€ HumanEval_Scenario_testcases.xlsx
â”‚   â”œâ”€â”€ MBPP_Scenario_testcases.xlsx
â”‚
â”œâ”€â”€ ğŸ“‚ LLM-Based Evaluation/                   # Multi-agent modules
â”‚   â”œâ”€â”€ mainchatgpt.py            # Test evaluation and test fixer agent
â”‚   â”œâ”€â”€ maindeepseek.py 
â”‚
â”œâ”€â”€ ğŸ“‚ Baseline/              # Evaluation and metric calculation
â”‚   â”œâ”€â”€ coverage_metrics.py
â”‚   â”œâ”€â”€ ddp_tce_metrics.py
â”‚   â”œâ”€â”€ maintainability.py
â”‚
â”œâ”€â”€ ğŸ“‚ ScenarioGeneration/              # Scenario generation
â”‚   â”œâ”€â”€ main.py
|
â”œâ”€â”€ .env                        # API Keys (OpenAI, DeepSeek)
â”œâ”€â”€ requirements.txt           # Required packages
â”œâ”€â”€ README.md                  # Documentation file
```

# ğŸ“Œ Requirements
ğŸ–¥ï¸ System

    Python 3.9+

    Internet access for LLM API calls

Install them with:

pip install -r requirements.txt

## Running TestCopilot

To run TestCopilot on your dataset, use the following command:

```bash
python scripts/run_testcopilot.py --input_dir path/to/your/input --output_dir path/to/save/results
```

