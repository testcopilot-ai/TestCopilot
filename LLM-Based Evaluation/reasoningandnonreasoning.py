import os
import pandas as pd
from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score
from openai import OpenAI

# =========================
# Configuration
# =========================
DEEPSEEK_API_KEY = os.getenv("API_KEY", "your_API_KEY")
DEEPSEEK_BASE_URL = os.getenv("LLMS_BASE_URL", "url")

INPUT_XLSX = r"path_to_directory/HumanEvalu_Scanerio_testcases.xlsx"
INPUT_SHEET = "Лист1"   # adjust if needed

N_SAMPLES = 50           # sample size
RANDOM_SEED = 42
MAX_RETRIES = 2          # repair cap to align with your paper policy

# Model IDs (edit if your account uses different names)
MODEL_NO_REASONING = "deepseek-chat"      # reasoning disabled
MODEL_REASONING    = "deepseek-reasoner"  # reasoning enabled

# Output paths
OUT_DIR = r"out_put_directory"
OUT_PREFIX = "HumanEvalu-Agent_Tools_AI_Evaluation_Results_reason_no_reason"

# =========================
# Client
# =========================
client = OpenAI(api_key=DEEPSEEK_API_KEY, base_url=DEEPSEEK_BASE_URL)

# =========================
# Agents
# =========================
def chat_with_deepseek(user_input: str, model: str):
    """Single-turn chat call to DeepSeek with the chosen model."""
    try:
        resp = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": "You are a helpful assistant."},
                {"role": "user",   "content": user_input},
            ],
            stream=False
        )
        return resp.choices[0].message.content
    except Exception as e:
        return f"Error: {e}"

def informant_agent(functional_requirement: str, scenario: str, testcase: str, code: str, model: str):
    """Verifies completeness/consistency of inputs."""
    if not all([functional_requirement, scenario, testcase, code]):
        return "Error: Missing required inputs.", False
    prompt = f"""
Validate the following inputs:
Functional Requirement: {functional_requirement}
Scenario: {scenario}
Test Case: {testcase}
Code: {code}

Check if all inputs are well-defined and consistent. Reply with "valid" if inputs are usable, otherwise explain the issue.
"""
    response = chat_with_deepseek(prompt, model=model)
    is_valid = isinstance(response, str) and ("valid" in response.lower() and "invalid" not in response.lower())
    return response, is_valid

def fixer_agent(functional_requirement: str, scenario: str, testcase: str, code: str, model: str):
    """Evaluates a test; if it fails, proposes a fix."""
    evaluation_prompt = f"""
Functional Requirement: {functional_requirement}
Scenario: {scenario}
Test Case: {testcase}
Code: {code}

Evaluate the following:
1. Read the functional requirement, Scanerio generated by self-RAG, further look at the test cases and code.
2. Evaluate testcases based on the functional requirement and self-RAG. Understand user intention and evaluate test cases based on the given code.
3. Does the test case adequately cover the functional requirement?
4. Specify what was the reason behind if the test case passes or fails.
5. If it fails, rewrite the correct code of the failing test case based on functional requirement and and ensure
    the logic is such that it would pass the corresponding tests provided
6. Evaluate code coverage (branch coverage, line coverage, functional coverage, Test Case Effectiveness, Defect Detection Percentage (DDP) 
    Function Coverage, Statement Coverage, Path Coverage, Shallow Coverage, Deep Coverage, Integration Coverage).
7. Evaluate test case effectiveness and give the total percentage of evaluation matrics above.
8.If starting code is provided look at the issues mentioned and attempt to fix them
9. Ensure that the signature of the function is the same as the one provided by the user
"""
    evaluation = chat_with_deepseek(evaluation_prompt, model=model)

    fixed = None
    if isinstance(evaluation, str) and ("fail" in evaluation.lower()):
        fix_prompt = f"""
The following test case and/or code have issues. Provide corrected TEST CASE ONLY.

Original Test Case:
{testcase}

Code Under Test:
{code}
"""
        fixed = chat_with_deepseek(fix_prompt, model=model)

    return evaluation, fixed

# =========================
# Metrics
# =========================
def compute_metrics(y_true, y_pred):
    """Compute precision, recall, F1, accuracy."""
    precision = precision_score(y_true, y_pred, average="binary", zero_division=1)
    recall    = recall_score(y_true, y_pred, average="binary", zero_division=1)
    f1        = f1_score(y_true, y_pred, average="binary", zero_division=1)
    accuracy  = accuracy_score(y_true, y_pred)
    return precision, recall, f1, accuracy

# =========================
# Experiment Runner
# =========================
def run_experiment(model_name: str, tag: str):
    """
    Runs informant+fixer pipeline on a 50-sample slice, capped to MAX_RETRIES.
    tag: used to differentiate outputs, e.g., 'reasoning_on' vs 'reasoning_off'
    """
    # Load & sample
    df = pd.read_excel(INPUT_XLSX, sheet_name=INPUT_SHEET)
    df.columns = df.columns.str.strip()

    # Column keys (adjust to your file if needed)
    col_req = "functional requirement"
    col_scn = "Scenarios"
    col_tst = "Testcases"
    col_code = "Code"

    # Defensive checks
    for c in [col_req, col_scn, col_tst, col_code]:
        if c not in df.columns:
            raise ValueError(f"Missing column: '{c}' in the input Excel sheet.")

    n = min(N_SAMPLES, len(df))
    sample_df = df.sample(n=n, random_state=RANDOM_SEED).reset_index(drop=True)

    results = []
    y_true, y_pred = [], []

    for idx, row in sample_df.iterrows():
        functional_requirement = row.get(col_req, "")
        scenario              = row.get(col_scn, "")
        testcase              = row.get(col_tst, "")
        code                  = row.get(col_code, "")

        # Informant validation
        informant_response, is_valid = informant_agent(
            functional_requirement, scenario, testcase, code, model=model_name
        )
        if not is_valid:
            results.append({
                "Row": idx,
                "Functional Requirement": functional_requirement,
                "Scenario": scenario,
                "Test Case": testcase,
                "Evaluation": informant_response,
                "Fixed Test Case": "",
                "Retries Used": 0,
                "Pass": 0
            })
            y_true.append(0)
            y_pred.append(0)
            continue

        # Fixer loop (max MAX_RETRIES)
        retries = 0
        passed = 0
        evaluation_txt = ""
        fixed_txt = ""

        while retries < MAX_RETRIES:
            evaluation, fixed = fixer_agent(
                functional_requirement, scenario, testcase, code, model=model_name
            )
            evaluation_txt = evaluation or ""
            fixed_txt = fixed or ""

            if isinstance(evaluation_txt, str) and ("pass" in evaluation_txt.lower()):
                passed = 1
                break
            retries += 1

            # Optionally, update testcase with the fixed suggestion for the next retry
            if fixed_txt and isinstance(fixed_txt, str):
                testcase = fixed_txt

        y_true.append(1 if passed else 0)
        y_pred.append(1 if passed else 0)

        results.append({
            "Row": idx,
            "Functional Requirement": functional_requirement,
            "Scenario": scenario,
            "Test Case": testcase,
            "Evaluation": evaluation_txt,
            "Fixed Test Case": fixed_txt,
            "Retries Used": retries,
            "Pass": passed
        })

    # Metrics
    precision, recall, f1, accuracy = compute_metrics(y_true, y_pred)
    summary = {
        "tag": tag,
        "model": model_name,
        "n_items": n,
        "precision": precision,
        "recall": recall,
        "f1": f1,
        "accuracy": accuracy,
        "pass_rate": sum(y_pred) / max(1, len(y_pred))
    }

    # Save detailed rows
    out_path = os.path.join(OUT_DIR, f"{OUT_PREFIX}_{tag}.xlsx")
    pd.DataFrame(results).to_excel(out_path, index=False)

    return summary, out_path

def main():
    os.makedirs(OUT_DIR, exist_ok=True)

    # Reasoning OFF
    summary_off, file_off = run_experiment(MODEL_NO_REASONING, tag="reasoning_off")

    # Reasoning ON
    summary_on, file_on = run_experiment(MODEL_REASONING, tag="reasoning_on")

    # Combine summaries
    summary_df = pd.DataFrame([summary_off, summary_on])
    summary_csv = os.path.join(OUT_DIR, f"{OUT_PREFIX}_summary_reasoning_compare.csv")
    summary_df.to_csv(summary_csv, index=False)

    print("=== Completed ===")
    print("Saved detailed outputs:")
    print(f"- {file_off}")
    print(f"- {file_on}")
    print("Summary:")
    print(summary_df.to_string(index=False))
    print(f"(CSV) {summary_csv}")

if __name__ == "__main__":
    main()
