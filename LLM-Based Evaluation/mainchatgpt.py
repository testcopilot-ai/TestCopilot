import os
import pandas as pd
from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score
from openai import OpenAI

# === Set OpenAI API credentials ===
openai_api_key = "your_openai_api_key_here"
openai_base_url = "https://api.openai.com/v1"
client = OpenAI(api_key=openai_api_key, base_url=openai_base_url)

# === Informant Agent ===
def informant_agent(functional_requirement, scenario, testcase, code):
    if not all([functional_requirement, scenario, testcase, code]):
        return "Error: Missing required inputs.", False
    prompt = f"""
    Validate the following inputs:
    Functional Requirement: {functional_requirement}
    Scenario: {scenario}
    Test Case: {testcase}
    buggy_Code: {buggy_code}

    Check if all inputs are well-defined and consistent.
    """
    response = chat_with_gpt4(prompt)
    return response, "valid" in response.lower()

# === Fixer Agent ===
def fixer_agent(functional_requirement, scenario, testcase, code):
    evaluation_prompt = f"""
    Functional Requirement: {functional_requirement}
    Scenario: {scenario}
    Test Case: {testcase}
    buggy_Code: {Buggy_code}

    Evaluate the following:
    1. Read the functional requirement, Scanerio generated by self-RAG, further look at the test cases and code.
    2. Evaluate testcases based on the functional requirement and self-RAG. Understand user intention and evaluate test cases based on the given code.
    3. Does the test case adequately cover the functional requirement?
    4. Specify what was the reason behind if the test case passes or fails.
    5. If it fails, rewrite the correct code of the failing test case based on functional requirement and and ensure
    the logic is such that it would pass the corresponding testcase provided
    4. Evaluate code coverage (branch coverage, line coverage, functional coverage, Test Case Effectiveness, Defect Detection Percentage (DDP) 
    Function Coverage, Statement Coverage, Path Coverage, Shallow Coverage, Deep Coverage, Integration Coverage).
    5. Evaluate test case effectiveness and give the total percentage of evaluation matrics above.
    6.If starting code is provided look at the issues mentioned and attempt to fix them
    7. Ensure that the signature of the function is the same as the one provided by the user
    """

    evaluation = chat_with_gpt4(evaluation_prompt)

    if "fail" in evaluation.lower():
        fix_prompt = f"""
        The following test case or code seems problematic. Suggest corrections:

        Test Case:
        {testcase}

        buggy_Code:
        {code}

        Provide fixed test cases with explanations.
        """
        fixed_response = chat_with_gpt4(fix_prompt)
        return evaluation, fixed_response

    return evaluation, None

# === GPT-4 Chat Helper ===
def chat_with_gpt4(user_input, model="gpt-4"):
    try:
        response = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": "You are a helpful assistant for software test evaluation."},
                {"role": "user", "content": user_input},
            ],
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        return f"Error: {e}"

# === Metric Calculation ===
def compute_metrics(y_true, y_pred):
    precision = precision_score(y_true, y_pred, average='binary', zero_division=1)
    recall = recall_score(y_true, y_pred, average='binary', zero_division=1)
    f1 = f1_score(y_true, y_pred, average='binary', zero_division=1)
    accuracy = accuracy_score(y_true, y_pred)
    return precision, recall, f1, accuracy

# === Main Entry ===
def main():
    # Load Dataset
    file_path = "mbpp_Scanerio_testcases.xlsx"
    df = pd.read_excel(file_path, sheet_name="Лист1")
    df.columns = df.columns.str.strip()

    results = []
    y_true = []
    y_pred = []
    max_retries = 3

    for index, row in df.iterrows():
        functional_requirement = row.get("functional requirement", "")
        scenario = row.get("Scenarios", "")
        testcase = row.get("Testcases", "")
        buggy_code = row.get("buggy_Code", "")

        # Informant Agent Check
        informant_response, is_valid = informant_agent(functional_requirement, scenario, testcase, code)
        if not is_valid:
            print(f"Row {index}: {informant_response}")
            results.append({
                "Functional Requirement": functional_requirement,
                "Scenario": scenario,
                "Test Case": testcase,
                "Evaluation": informant_response,
                "Fixed Test Case": "N/A",
                
            })
            y_pred.append(0)
            y_true.append(0)
            continue

        # Fixer Agent Logic
        retries = 0
        while retries < max_retries:
            evaluation, fixed_response = fixer_agent(functional_requirement, scenario, testcase, code)
            print(f"Evaluation for row {index}: {evaluation}")

            if "pass" in evaluation.lower():
                y_pred.append(1)
                y_true.append(1)
                results.append({
                    "Functional Requirement": functional_requirement,
                    "Scenario": scenario,
                    "Test Case": testcase,
                    "Evaluation": evaluation,
                    "Fixed Test Case": "N/A",
                    
                })
                break
            else:
                y_pred.append(0)
                y_true.append(0)
                retries += 1
                results.append({
                    "Functional Requirement": functional_requirement,
                    "Scenario": scenario,
                    "Test Case": testcase,
                    "Evaluation": evaluation,
                    "Fixed Test Case": fixed_response
                })

                if retries == max_retries:
                    print(f"Max retries reached for row {index}")

    # Save results
    precision, recall, f1, accuracy = compute_metrics(y_true, y_pred)
    results_df = pd.DataFrame(results)
    output_path = "mbpp-Agent_Tools_AI_Evaluation_Results.xlsx"
    try:
        results_df.to_excel(output_path, index=False)
        print(f"Results saved to {output_path}")
    except Exception as e:
        print(f"Error saving results: {e}")

    print(f"Metrics:\n Precision={precision:.2f}, Recall={recall:.2f}, F1 Score={f1:.2f}, Accuracy={accuracy:.2f}")

if __name__ == "__main__":
    main()
